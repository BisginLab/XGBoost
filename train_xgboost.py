# -*- coding: utf-8 -*-
"""Bisgin_Paper_PyFile.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hGYdWMVD5setW3SNQVujIiFAYZQk8fN6
"""

import zipfile
import io
import pandas as pd
import numpy as np
import sys
from datetime import datetime
import json

import xgboost as xgb
import torch
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
import os
import joblib

def encode_and_bind(original_dataframe, feature_to_encode, categories=None):
    """One-hot encode a categorical feature and bind it to the original dataframe"""
    dummies = pd.get_dummies(original_dataframe[[feature_to_encode]], prefix=feature_to_encode)
    if categories is not None:
        # Ensure all categories are present
        for cat in categories:
            col_name = f"{feature_to_encode}_{cat}"
            if col_name not in dummies.columns:
                dummies[col_name] = 0
        # Reorder columns to match categories
        dummies = dummies.reindex(columns=[f"{feature_to_encode}_{cat}" for cat in categories])
    return dummies

# Check XGBoost GPU support
print("XGBoost GPU support:", xgb.build_info())

# Check available GPUs
print("CUDA available:", torch.cuda.is_available())
print("CUDA device count:", torch.cuda.device_count())

if torch.cuda.is_available():
    for i in range(torch.cuda.device_count()):
        print(f"GPU {i}: {torch.cuda.get_device_name(i)}")

# TO DO
# double check user vs developer centric features from paper
# double check validation and training/testing split
# do evaluation for BOTH user and developer centered
# Explaining misclassifications with llm
# remove unnnamed index
# compare earlier dataset with corrected dataset
# make colab copy to folderr
# push progress to github repo
# do google slides for excelformer, add to google drive

# Add at the start of the script, after imports
class Logger:
    def __init__(self, filename):
        self.terminal = sys.stdout
        self.log = open(filename, 'w')

    def write(self, message):
        self.terminal.write(message)
        self.log.write(message)
        self.log.flush()

    def flush(self):
        self.terminal.flush()
        self.log.flush()

# Add after the imports
import sys
timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
log_filename = f'training_log_{timestamp}.txt'
sys.stdout = Logger(log_filename)

# Read the CSV file
print("Loading data...")
df = pd.read_csv('./content/sample_data/corrected_permacts.csv')
print(f"Initial DataFrame shape: {df.shape}")

# Drop NaNs first, but preserve the original index
df = df.dropna(ignore_index=False)
print(f"Shape after dropping NaNs: {df.shape}")

# Define features based on MI ranking
selected_features = [
    'ContentRating', 'Genre', 'CurrentVersion', 'AndroidVersion', 
    'DeveloperCategory', 'lowest_android_version', 'highest_android_version',
    'privacy_policy_link', 'developer_website', 'days_since_last_update',
    'isSpamming', 'max_downloads_log', 'LenWhatsNew', 'PHONE',
    'OneStarRatings', 'developer_address', 'FourStarRatings', 'intent',
    'ReviewsAverage', 'STORAGE', 'LastUpdated', 'TwoStarRatings',
    'LOCATION', 'FiveStarRatings', 'ThreeStarRatings'
]

# Define categorical features
categorical_features = [
    'ContentRating', 'highest_android_version', 'CurrentVersion',
    'lowest_android_version', 'AndroidVersion', 'DeveloperCategory', 'Genre'
]

# Get numerical features
numerical_features = [f for f in selected_features if f not in categorical_features]

# After feature definitions but before training
print("\nTraining with these features:")
print("\nNumerical features:")
for i, feat in enumerate(numerical_features, 1):
    print(f"{i}. {feat}")

print("\nCategorical features:")
for i, feat in enumerate(categorical_features, 1):
    print(f"{i}. {feat}")

print("\nTotal feature count:", len(numerical_features) + len(categorical_features))
print("\nFeatures to be used in training:")
for i, feat in enumerate(selected_features, 1):
    print(f"{i}. {feat}")

print("\nStarting training...")

# Prepare X and y
X = df[selected_features]
y = df['status']

print("\nFeature counts:")
print(f"Total selected features: {len(selected_features)}")
print(f"Numerical features: {len(numerical_features)}")
print(f"Categorical features: {len(categorical_features)}")

# Load the indices that were created after dropping NaNs
train_indices = np.load('./content/sample_data/train_indices.npy')
val_indices = np.load('./content/sample_data/val_indices.npy')
test_indices = np.load('./content/sample_data/test_indices.npy')

# After loading indices but before splitting
print("\nIndices info:")
print(f"Number of train indices: {len(train_indices)}")
print(f"Number of val indices: {len(val_indices)}")
print(f"Number of test indices: {len(test_indices)}")
print(f"Min/Max train indices: {train_indices.min()}, {train_indices.max()}")
print(f"Min/Max val indices: {val_indices.min()}, {val_indices.max()}")
print(f"Min/Max test indices: {test_indices.min()}, {test_indices.max()}")

# Create validation and test sets
X_val = X.loc[val_indices]
y_val = y.loc[val_indices]
X_test = X.loc[test_indices]
y_test = y.loc[test_indices]

# Create preprocessing pipeline
preprocessor = ColumnTransformer(
    transformers=[
        ('num', 'passthrough', numerical_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
    ])

# Create XGBoost pipeline
xgb_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', xgb.XGBClassifier(
        n_estimators=1000,
        max_depth=6,
        learning_rate=0.01,
        subsample=0.8,
        colsample_bytree=0.8,
        min_child_weight=1,
        gamma=0,
        enable_categorical=True,
        device='cuda:0' if torch.cuda.is_available() else 'cpu',
        n_jobs=-1,
        max_bin=256
    ))
])

# Create directory for sample indices if it doesn't exist
os.makedirs('sample_indices', exist_ok=True)

# Create fixed sample sizes for consistent training across models
sample_sizes = [10000, 50000, 100000]
np.random.seed(42)  # Set random seed for reproducibility

# Training loop
for size in sample_sizes:
    print(f"\n{'='*50}")
    print(f"Training with sample size: {size}")
    print(f"{'='*50}\n")
    
    # Load or create sample indices with train/val/test split
    train_indices_file = f'sample_indices/train_indices_{size}.npy'
    val_indices_file = f'sample_indices/val_indices_{size}.npy'
    test_indices_file = f'sample_indices/test_indices_{size}.npy'
    
    if os.path.exists(train_indices_file) and os.path.exists(val_indices_file) and os.path.exists(test_indices_file):
        # Load existing splits
        train_sample_indices = np.load(train_indices_file)
        val_sample_indices = np.load(val_indices_file)
        test_sample_indices = np.load(test_indices_file)
        print(f"\nLoaded existing train/val/test splits for {size} samples")
    else:
        # Create new splits
        # First sample from the full training set
        sample_indices = np.random.choice(train_indices, size=size, replace=False)
        
        # Split into train/val/test (70/15/15)
        train_size = int(size * 0.7)
        val_size = int(size * 0.15)
        
        train_sample_indices = sample_indices[:train_size]
        val_sample_indices = sample_indices[train_size:train_size + val_size]
        test_sample_indices = sample_indices[train_size + val_size:]
        
        # Save the splits
        np.save(train_indices_file, train_sample_indices)
        np.save(val_indices_file, val_sample_indices)
        np.save(test_indices_file, test_sample_indices)
        print(f"\nCreated and saved train/val/test splits for {size} samples")
    
    print(f"\nTraining with {size} samples")
    print(f"Train set size: {len(train_sample_indices)}")
    print(f"Validation set size: {len(val_sample_indices)}")
    print(f"Test set size: {len(test_sample_indices)}")
    
    # Create sampled training set
    X_train_sampled = X.loc[train_sample_indices]
    y_train_sampled = y.loc[train_sample_indices]
    
    # Create sampled validation and test sets
    X_val_sampled = X.loc[val_sample_indices]
    y_val_sampled = y.loc[val_sample_indices]
    X_test_sampled = X.loc[test_sample_indices]
    y_test_sampled = y.loc[test_sample_indices]
    
    # Train ensemble of models
    models = []
    for i in range(11):  # Train 11 models
        print(f"\nTraining model {i+1}/11")
        model = xgb_pipeline.set_params(
            classifier__n_estimators=1000,
            classifier__max_depth=6,
            classifier__learning_rate=0.01,
            classifier__subsample=0.8,
            classifier__colsample_bytree=0.8,
            classifier__min_child_weight=1,
            classifier__gamma=0,
            classifier__random_state=42 + i
        )
        
        model.fit(X_train_sampled, y_train_sampled)
        models.append(model)
    
    # Evaluate models
    def evaluate_models(models, X, y, set_name):
        try:
            # Get predictions from all models
            y_preds_proba = []
            for model in models:
                # Get preprocessor and classifier separately
                preprocessor = model.named_steps['preprocessor']
                classifier = model.named_steps['classifier']
                
                # Transform data first
                X_transformed = preprocessor.transform(X)
                
                # Then predict with classifier directly
                classifier.set_params(device='cpu')  # Force CPU prediction
                y_pred = classifier.predict_proba(X_transformed)[:, 1]
                y_preds_proba.append(y_pred)
            
            y_pred_proba = np.mean(y_preds_proba, axis=0)
            
            # Calculate metrics
            fpr, tpr, _ = roc_curve(y, y_pred_proba)
            auc_score = auc(fpr, tpr)
            
            y_pred = (y_pred_proba > 0.5).astype(int)
            accuracy = np.mean(y_pred == y)
            
            print(f'\n{set_name.upper()} SET METRICS:')
            print(f'ROC AUC: {auc_score:.4f}')
            print(f'Accuracy: {accuracy:.4f}')
            
            return {'fpr': fpr, 'tpr': tpr, 'auc': auc_score, 'predictions': y_pred_proba}
        except Exception as e:
            print(f"Error in evaluate_models: {str(e)}")
            raise
    
    val_metrics = evaluate_models(models, X_val_sampled, y_val_sampled, "VALIDATION SET")
    test_metrics = evaluate_models(models, X_test_sampled, y_test_sampled, "TEST SET")
    
    # Save models
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    model_filename = f'saved_models/xgboost_ensemble_{size}_run_{timestamp}.joblib'
    joblib.dump(models, model_filename)
    print(f"\nSaved trained models as: {model_filename}")
    
    # Calculate and plot ROC curve
    plt.figure(figsize=(10, 8))
    plt.plot(val_metrics['fpr'], val_metrics['tpr'], 
             label=f'Validation (AUC = {val_metrics["auc"]:.4f})', color='blue')
    plt.plot(test_metrics['fpr'], test_metrics['tpr'], 
             label=f'Test (AUC = {test_metrics["auc"]:.4f})', color='red')
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'XGBoost ROC Curve - {size} Samples')
    plt.legend()
    plt.grid(True)
    
    # Save plot
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    plot_filename = f'xgboost_roc_curve_sample_{size}_{timestamp}.png'
    plt.savefig(plot_filename)
    plt.close()
    print(f"\nSaved ROC curve plot as: {plot_filename}")
    
    # Calculate feature importance
    print("\nCalculating feature importance from trained models...")
    # Get feature names from preprocessor
    preprocessor = models[0].named_steps['preprocessor']
    cat_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)
    feature_names = numerical_features + list(cat_feature_names)
    
    feature_importance = np.zeros(len(feature_names))
    for model in models:
        feature_importance += model.named_steps['classifier'].feature_importances_
    feature_importance /= len(models)
    
    # Create feature importance DataFrame
    importance_df = pd.DataFrame({
        'Feature': feature_names,
        'Importance': feature_importance
    })
    importance_df = importance_df.sort_values('Importance', ascending=False)
    
    # Save feature importance to file
    importance_filename = f'xgboost_feature_importance_sample_{size}_{timestamp}.txt'
    with open(importance_filename, 'w') as f:
        f.write("Top 25 Features by Importance:\n")
        for i, (feature, importance) in enumerate(zip(importance_df['Feature'], importance_df['Importance']), 1):
            f.write(f"{i}. {feature}: {importance:.4f}\n")
    print(f"Saved feature importance to: {importance_filename}")
    
    # Plot feature importance
    plt.figure(figsize=(12, 8))
    plt.barh(range(25), importance_df['Importance'][:25])
    plt.yticks(range(25), importance_df['Feature'][:25])
    plt.xlabel('Importance')
    plt.title(f'XGBoost - Top 25 Feature Importance - {size} Samples')
    plt.tight_layout()
    
    # Save plot
    plot_filename = f'xgboost_feature_importance_sample_{size}_{timestamp}.png'
    plt.savefig(plot_filename)
    plt.close()
    print(f"Saved feature importance plot as: {plot_filename}")

# Save training log
with open(f'xgboost_training_log_{datetime.now().strftime("%Y%m%d_%H%M%S")}.txt', 'w') as f:
    f.write("Training completed successfully\n")
print(f"Training log saved to: xgboost_training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt")

# FEATURE IMPORTANCE
# After training models...
print("\nCalculating feature importance from trained models...")
# Get the correct shape from the first model
first_model = models[0]
xgb_feature_importance = first_model.named_steps['classifier'].feature_importances_
feature_importance = np.zeros_like(xgb_feature_importance)

for model in models:
    # Get feature importance from the XGBoost classifier
    xgb_feature_importance = model.named_steps['classifier'].feature_importances_
    # Accumulate feature importance
    feature_importance += xgb_feature_importance

# Average feature importance across all models
feature_importance /= len(models)

# Get feature names from the encoded data
feature_names = X_train_sampled.columns.tolist()

# Create a dictionary of feature importances
feature_importance_dict = dict(zip(feature_names, feature_importance))

# Sort features by importance
sorted_features = sorted(feature_importance_dict.items(), 
                        key=lambda x: x[1], 
                        reverse=True)

# Print top 25 features with their importance (non-aggregated)
print("\nTop 25 Features by Importance (Non-aggregated):")
for i, (feature, importance) in enumerate(sorted_features[:25], 1):
    print(f"{i}. {feature}: {importance:.4f}")

# Aggregate feature importance for categorical features
aggregated_importance = {}
for feature, importance in feature_importance_dict.items():
    # Extract base feature name (remove the _category suffix)
    base_feature = feature.split('_')[0]
    if base_feature in categorical_features:
        if base_feature not in aggregated_importance:
            aggregated_importance[base_feature] = 0
        aggregated_importance[base_feature] += importance
    else:
        # For non-categorical features, keep as is
        aggregated_importance[feature] = importance

# Sort aggregated features by importance
sorted_aggregated = sorted(aggregated_importance.items(), 
                         key=lambda x: x[1], 
                         reverse=True)

# Print top 25 aggregated features
print("\nTop 25 Features by Importance (Aggregated):")
for i, (feature, importance) in enumerate(sorted_aggregated[:25], 1):
    print(f"{i}. {feature}: {importance:.4f}")

# Save feature importance to file
timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
filename = f'feature_importance_{timestamp}.txt'
with open(filename, 'w') as f:
    f.write("Non-aggregated Feature Importance:\n")
    for feature, importance in sorted_features:
        f.write(f"{feature}: {importance:.4f}\n")
    f.write("\nAggregated Feature Importance:\n")
    for feature, importance in sorted_aggregated:
        f.write(f"{feature}: {importance:.4f}\n")
print(f"\nSaved feature importance to: {filename}")

# Plot feature importance (non-aggregated)
plt.figure(figsize=(12, 10))
top_n = 25
top_features = [x[0] for x in sorted_features[:top_n]]
top_importance = [x[1] for x in sorted_features[:top_n]]

plt.barh(range(len(top_features)), top_importance)
plt.yticks(range(len(top_features)), top_features)
plt.xlabel('Feature Importance')
plt.title(f'Top 25 Features by Importance (Non-aggregated)')
plt.tight_layout()

# Save non-aggregated plot
plot_filename = f'feature_importance_plot_{timestamp}.png'
plt.savefig(plot_filename, dpi=300, bbox_inches='tight')
plt.close()
print(f"Saved non-aggregated feature importance plot as: {plot_filename}")

# Plot aggregated feature importance
plt.figure(figsize=(12, 10))
top_agg_features = [x[0] for x in sorted_aggregated[:top_n]]
top_agg_importance = [x[1] for x in sorted_aggregated[:top_n]]

plt.barh(range(len(top_agg_features)), top_agg_importance)
plt.yticks(range(len(top_agg_features)), top_agg_features)
plt.xlabel('Feature Importance')
plt.title('Top 25 Features by Importance (Aggregated)')
plt.tight_layout()

# Save aggregated plot
plot_filename = f'feature_importance_plot_aggregated_{timestamp}.png'
plt.savefig(plot_filename, dpi=300, bbox_inches='tight')
plt.close()
print(f"Saved aggregated feature importance plot as: {plot_filename}")

# LLM-BASED PREDICTION EXPLANATION
'''
!pip install groq
import base64
import groq
import asyncio
import os

def visualize_decision_path(models, sample_idx=0):
    """Create a readable decision tree visualization by setting feature names directly"""
    model = models[0]

    # Get feature names
    preprocessor = models[0]
    cat_feature_names = preprocessor.get_booster().feature_names
    feature_names = numerical_features + list(cat_feature_names)

    # Set feature names directly on the booster
    model.get_booster().feature_names = feature_names

    plt.figure(figsize=(25, 15))
    plot_tree(model, num_trees=0)
    plt.tight_layout()
    return plt

# Run visualization
plt = visualize_decision_path(models)
plt.show()

def create_feature_contribution_map(models, X_sample):
    """Create a more readable feature contribution heatmap"""
    # Get feature contributions for each model
    contributions = []

    for model in models:
        X_transformed = model.transform(X_sample)
        contribution = model.feature_importances_
        contributions.append(contribution)

    # Average contributions across models
    avg_contribution = np.mean(contributions, axis=0)

    # Get feature names
    preprocessor = models[0]
    cat_feature_names = preprocessor.get_booster().feature_names
    feature_names = numerical_features + list(cat_feature_names)

    # Sort features by contribution magnitude
    sorted_indices = np.argsort(np.abs(avg_contribution))
    top_n = 20  # Show only top 20 most important features

    selected_indices = sorted_indices[-top_n:]
    selected_contributions = avg_contribution[selected_indices]
    selected_features = [feature_names[i] for i in selected_indices]

    # Create contribution map
    plt.figure(figsize=(15, 8))
    plt.barh(range(len(selected_contributions)), selected_contributions)

    # Customize appearance
    plt.yticks(range(len(selected_features)), selected_features, fontsize=10)
    plt.xlabel('Feature Contribution', fontsize=12)
    plt.title('Top Feature Contributions', fontsize=14)
    plt.grid(axis='x', linestyle='--', alpha=0.7)

    # Color positive and negative contributions differently
    colors = ['red' if x < 0 else 'blue' for x in selected_contributions]
    plt.barh(range(len(selected_contributions)), selected_contributions, color=colors)

    # Add value labels on the bars
    for i, v in enumerate(selected_contributions):
        plt.text(v, i, f'{v:.3f}',
                va='center',
                fontsize=8,
                fontweight='bold')

    plt.tight_layout()
    return plt

# Generate and visualize feature contribution map
plt = create_feature_contribution_map(models, X_test.iloc[[0]])
plt.show()

import google.generativeai as genai
from google.colab import userdata
genai.configure(api_key=userdata.get('GEMINI_API_KEY'))

def visualize_decision_path(models, sample_idx):
    """Create a readable decision tree visualization by setting feature names directly"""
    model = models[0]

    # Get feature names
    preprocessor = models[0]
    cat_feature_names = preprocessor.get_booster().feature_names
    feature_names = numerical_features + list(cat_feature_names)

    # Set feature names directly on the booster
    model.get_booster().feature_names = feature_names

    plt.figure(figsize=(25, 15))
    plot_tree(model, num_trees=0)
    plt.tight_layout()
    return plt

def save_visualizations(models, X_sample, sample_idx, output_dir='explanation_plots'):
    """Save decision tree and feature importance visualizations"""
    os.makedirs(output_dir, exist_ok=True)

    # Generate and save decision tree plot
    plt = visualize_decision_path(models, sample_idx)
    tree_path = os.path.join(output_dir, 'decision_tree.png')
    plt.savefig(tree_path)
    plt.close()

    # Generate and save feature contribution map
    plt = create_feature_contribution_map(models, X_sample)
    contrib_path = os.path.join(output_dir, 'feature_contribution.png')
    plt.savefig(contrib_path)
    plt.close()

    # Get feature names
    preprocessor = models[0]
    cat_feature_names = preprocessor.get_booster().feature_names
    feature_names = numerical_features + list(cat_feature_names)

    return {
        'decision_tree': tree_path,
        'contribution_map': contrib_path
    }, feature_names

def generate_model_explanation(models, X_sample, sample_idx):
    """Generate explanation using Gemini Flash"""
    from google.generativeai import GenerativeModel
    import PIL.Image

    # Generate and save visualizations
    viz_paths, feature_names = save_visualizations(models, X_sample, sample_idx)

    # Get prediction
    y_pred = np.mean([model.predict_proba(X_sample)[:, 1] for model in models])

    # Get feature contributions
    contributions = []
    for model in models:
        X_transformed = model.transform(X_sample)
        contribution = model.feature_importances_
        contributions.append(contribution)
    avg_contribution = np.mean(contributions, axis=0)

    # Get top features
    sorted_features = sorted(
        zip(feature_names, avg_contribution),
        key=lambda x: abs(x[1]),
        reverse=True
    )[:5]

    # Create prompt
    prompt = f"""You are an expert ML model interpreter. You're tasked with explaining an XGBoost model's decision
    path and feature importance plots for malware detection, where the model predicts if an app is malicious (1) or benign (0).

    The model predicted a score of {y_pred:.3f} (higher values indicate higher likelihood of malware).

    The top 5 most influential features were:
    {sorted_features}

    In your response:
    - Explain what features the model is focusing on most heavily based on the feature importance plot
    - Explain what decision path the model took to reach its prediction, based on the decision tree visualization
    - Explain possible reasons why the model made the prediction it did
    - Keep your explanation to 4 sentences max"""

    # Load images
    tree_img = PIL.Image.open(viz_paths['decision_tree'])
    feat_img = PIL.Image.open(viz_paths['contribution_map'])

    # Get explanation from Gemini
    model = GenerativeModel(model_name='gemini-1.5-flash')
    response = model.generate_content([prompt, tree_img, feat_img])

    return response.text, y_pred

def explain_prediction(models, X_sample, sample_idx):
    """Wrapper to get and print model explanation"""
    try:
        explanation, y_pred = generate_model_explanation(models, X_sample, sample_idx)
        print(f"Prediction: {y_pred:.3f}")
        print("\nModel Explanation:")
        print("-" * 80)
        # print(explanation)
        return explanation, y_pred
    except Exception as e:
        print(f"Error getting explanation: {e}")
        return None, None

# Get explanation for a sample
sample_idx = 1
# print(X_train.iloc[sample_idx])
X_sample = X_test.iloc[[sample_idx]]
explain_prediction(models, X_sample, sample_idx)
'''

# Add at the very end of the script
sys.stdout = sys.stdout.terminal  # Restore normal stdout
print(f"Training log saved to: {log_filename}")